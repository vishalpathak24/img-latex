{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "blUrp84zGByT",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7d2124e9-7758-4c5e-e4d5-1d48028f2638"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'img-latex'...\n",
      "remote: Enumerating objects: 19, done.\u001b[K\n",
      "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
      "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
      "remote: Total 19 (delta 1), reused 19 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (19/19), 8.92 KiB | 8.92 MiB/s, done.\n",
      "Resolving deltas: 100% (1/1), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/vishalpathak24/img-latex.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install 'transformers[torch]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ym4eYMaHJMUl",
    "outputId": "9b96517d-4b54-465c-af22-05a9a474fec6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torchserve torch-model-archiver torch-workflow-archiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/img-latex/img-latex\n"
     ]
    }
   ],
   "source": [
    "%cd img-latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDdisCNtJPys",
    "outputId": "d7b17392-5ded-4e33-dd8c-7dd26be916c3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/img-latex/img-latex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-0h4fyCaNB8G",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel\n",
    "from transformers.models.nougat import NougatTokenizerFast\n",
    "from nougat_latex import NougatLaTexProcessor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1f2okLxWM0DY"
   },
   "source": [
    "# Model Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tU6HTjbDMziv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"Norm/nougat-latex-base\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "tokenizer = NougatTokenizerFast.from_pretrained(model_name)\n",
    "latex_processor = NougatLaTexProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4XAfUA8POwgT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image = Image.open(\"../sample-images/lt-2.jpg\")\n",
    "if not image.mode == \"RGB\":\n",
    "    image = image.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "uZUpOBjvUL41",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pixel_values = latex_processor(image, return_tensors=\"pt\").pixel_values\n",
    "decoder_input_ids = tokenizer(tokenizer.bos_token, add_special_tokens=False,\n",
    "                              return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "N_0eiP9CURaf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        pixel_values.to(device),\n",
    "        decoder_input_ids=decoder_input_ids.to(device),\n",
    "        max_length=model.decoder.config.max_length,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "        num_beams=5,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]],\n",
    "        return_dict_in_generate=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_LQHaQV0UUI3",
    "outputId": "bb926135-8676-483b-a1e3-fde63cf915fe",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\textstyle{\\frac{1}{3}}\\times\\left(6\\times{\\frac{4}{3}}\\right)\n"
     ]
    }
   ],
   "source": [
    "sequence = tokenizer.batch_decode(outputs.sequences)[0]\n",
    "sequence = sequence.replace(tokenizer.eos_token, \"\").replace(tokenizer.pad_token, \"\").replace(tokenizer.bos_token, \"\")\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUIg0_EULp0r"
   },
   "source": [
    "# Creating Mar for inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "pwp2UYMqLo9V",
    "outputId": "e9939ffb-df4e-4c60-ce1d-8cd63002b16e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/studio-lab-user/sagemaker-studiolab-notebooks/img-latex/img-latex/nougat.zip'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive('nougat', 'zip', 'nougat_latex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7sm_UUvKMODu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "to8q2DwEUu8b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!torch-model-archiver --model-name img-latex \\\n",
    "--version 1.0 --model-file model.py \\\n",
    "--serialized-file model.pt \\\n",
    "--handler handler.py \\\n",
    "--extra-files \"nougat.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fhauiSnBVqDi",
    "outputId": "9d0190f5-6950-4cd9-fc01-649e92c4e01c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir ../../model-store/\n",
    "!mv img-latex.mar ../../model-store/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchserve in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (0.12.0)\n",
      "Requirement already satisfied: packaging in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torchserve) (24.0)\n",
      "Requirement already satisfied: psutil in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torchserve) (6.1.0)\n",
      "Requirement already satisfied: Pillow in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torchserve) (11.0.0)\n",
      "Requirement already satisfied: wheel in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torchserve) (0.43.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "04-y7MqLU_Hx",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "badab015-c52f-45a8-de08-7e9df7c112d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2024-12-16T12:13:38,056 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\n",
      "nvidia-smi not available or failed: Cannot run program \"nvidia-smi\": error=2, No such file or directory\n",
      "2024-12-16T12:13:38,147 [DEBUG] main org.pytorch.serve.util.ConfigManager - xpu-smi not available or failed: Cannot run program \"xpu-smi\": error=2, No such file or directory\n",
      "2024-12-16T12:13:38,151 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties\n",
      "2024-12-16T12:13:38,227 [INFO ] main org.pytorch.serve.util.TokenAuthorization - \n",
      "######\n",
      "TorchServe now enforces token authorization by default.\n",
      "This requires the correct token to be provided when calling an API.\n",
      "Key file located at /content/img-latex/img-latex/key_file.json\n",
      "Check token authorization documenation for information: https://github.com/pytorch/serve/blob/master/docs/token_authorization_api.md \n",
      "######\n",
      "\n",
      "2024-12-16T12:13:38,227 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\n",
      "2024-12-16T12:13:38,396 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /usr/local/lib/python3.10/dist-packages/ts/configs/metrics.yaml\n",
      "2024-12-16T12:13:38,525 [INFO ] main org.pytorch.serve.ModelServer - \n",
      "Torchserve version: 0.12.0\n",
      "TS Home: /usr/local/lib/python3.10/dist-packages\n",
      "Current directory: /content/img-latex/img-latex\n",
      "Temp directory: /tmp\n",
      "Metrics config path: /usr/local/lib/python3.10/dist-packages/ts/configs/metrics.yaml\n",
      "Number of GPUs: 0\n",
      "Number of CPUs: 2\n",
      "Max heap size: 3246 M\n",
      "Python executable: /usr/bin/python3\n",
      "Config file: N/A\n",
      "Inference address: http://127.0.0.1:8080\n",
      "Management address: http://127.0.0.1:8081\n",
      "Metrics address: http://127.0.0.1:8082\n",
      "Model Store: /content/model-store\n",
      "Initial Models: img-latex=img-latex.mar\n",
      "Log dir: /content/img-latex/img-latex/logs\n",
      "Metrics dir: /content/img-latex/img-latex/logs\n",
      "Netty threads: 0\n",
      "Netty client threads: 0\n",
      "Default workers per model: 2\n",
      "Blacklist Regex: N/A\n",
      "Maximum Response Size: 6553500\n",
      "Maximum Request Size: 6553500\n",
      "Limit Maximum Image Pixels: true\n",
      "Prefer direct buffer: false\n",
      "Allowed Urls: [file://.*|http(s)?://.*]\n",
      "Custom python dependency for model allowed: false\n",
      "Enable metrics API: true\n",
      "Metrics mode: LOG\n",
      "Disable system metrics: false\n",
      "Workflow Store: /content/model-store\n",
      "CPP log config: N/A\n",
      "Model config: N/A\n",
      "System metrics command: default\n",
      "Model API enabled: false\n",
      "2024-12-16T12:13:38,554 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: img-latex.mar\n",
      "2024-12-16T12:13:38,585 [INFO ] main org.pytorch.serve.archive.model.ModelArchive - createTempDir /tmp/models/404702df3aea4c89900d0c102a673075\n",
      "2024-12-16T12:13:38,586 [WARN ] main org.pytorch.serve.ModelServer - Failed to load model: img-latex.mar\n",
      "org.pytorch.serve.archive.model.ModelNotFoundException: Model not found at: img-latex.mar\n",
      "\tat org.pytorch.serve.archive.model.ModelArchive.downloadModel(ModelArchive.java:118) ~[model-server.jar:?]\n",
      "\tat org.pytorch.serve.wlm.ModelManager.createModelArchive(ModelManager.java:185) ~[model-server.jar:?]\n",
      "\tat org.pytorch.serve.wlm.ModelManager.registerModel(ModelManager.java:143) ~[model-server.jar:?]\n",
      "\tat org.pytorch.serve.ModelServer.initModelStore(ModelServer.java:266) [model-server.jar:?]\n",
      "\tat org.pytorch.serve.ModelServer.startRESTserver(ModelServer.java:399) [model-server.jar:?]\n",
      "\tat org.pytorch.serve.ModelServer.startAndWait(ModelServer.java:124) [model-server.jar:?]\n",
      "\tat org.pytorch.serve.ModelServer.main(ModelServer.java:105) [model-server.jar:?]\n",
      "2024-12-16T12:13:38,609 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "2024-12-16T12:13:40,790 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.\n",
      "java.io.IOException: Failed to bind to address: http://127.0.0.1:8080\n",
      "\tat org.pytorch.serve.ModelServer.initializeServer(ModelServer.java:354)\n",
      "\tat org.pytorch.serve.ModelServer.startRESTserver(ModelServer.java:415)\n",
      "\tat org.pytorch.serve.ModelServer.startAndWait(ModelServer.java:124)\n",
      "\tat org.pytorch.serve.ModelServer.main(ModelServer.java:105)\n",
      "Caused by: io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Address already in use\n"
     ]
    }
   ],
   "source": [
    "!torchserve --model-store model-store/ --models img-latex=img-latex.mar --ts-config img-latex/config.properties &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchServe has stopped.\n"
     ]
    }
   ],
   "source": [
    "!torchserve --stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "meo1oZdgZDV9",
    "outputId": "180be420-5915-4c9e-bd4e-72c888675623"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_3168988d-414d-41d4-aafa-72605550a462\", \"img-latex.mar\", 1293241540)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pip install captum"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".conda-default:Python",
   "language": "python",
   "name": "conda-env-.conda-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
