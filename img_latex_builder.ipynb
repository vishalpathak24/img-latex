{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "blUrp84zGByT",
    "outputId": "7d2124e9-7758-4c5e-e4d5-1d48028f2638"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/vishalpathak24/img-latex.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install 'transformers[torch]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ym4eYMaHJMUl",
    "outputId": "9b96517d-4b54-465c-af22-05a9a474fec6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torchserve torch-model-archiver torch-workflow-archiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/img-latex/img-latex\n"
     ]
    }
   ],
   "source": [
    "%cd img-latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDdisCNtJPys",
    "outputId": "d7b17392-5ded-4e33-dd8c-7dd26be916c3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/sagemaker-studiolab-notebooks/img-latex\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-0h4fyCaNB8G",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel\n",
    "from transformers.models.nougat import NougatTokenizerFast\n",
    "from nougat_latex import NougatLaTexProcessor\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1f2okLxWM0DY"
   },
   "source": [
    "# Model Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tU6HTjbDMziv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"Norm/nougat-latex-base\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "tokenizer = NougatTokenizerFast.from_pretrained(model_name)\n",
    "latex_processor = NougatLaTexProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4XAfUA8POwgT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image = Image.open(\"../sample-images/lt-2.jpg\")\n",
    "if not image.mode == \"RGB\":\n",
    "    image = image.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZUpOBjvUL41",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pixel_values = latex_processor(image, return_tensors=\"pt\").pixel_values\n",
    "decoder_input_ids = tokenizer(tokenizer.bos_token, add_special_tokens=False,\n",
    "                              return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_0eiP9CURaf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        pixel_values.to(device),\n",
    "        decoder_input_ids=decoder_input_ids.to(device),\n",
    "        max_length=model.decoder.config.max_length,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "        num_beams=5,\n",
    "        bad_words_ids=[[tokenizer.unk_token_id]],\n",
    "        return_dict_in_generate=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_LQHaQV0UUI3",
    "outputId": "bb926135-8676-483b-a1e3-fde63cf915fe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequence = tokenizer.batch_decode(outputs.sequences)[0]\n",
    "sequence = sequence.replace(tokenizer.eos_token, \"\").replace(tokenizer.pad_token, \"\").replace(tokenizer.bos_token, \"\")\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Testing Basic inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.donut.modeling_donut_swin.DonutSwinModel'> is overwritten by shared encoder config: DonutSwinConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"depths\": [\n",
      "    2,\n",
      "    2,\n",
      "    14,\n",
      "    2\n",
      "  ],\n",
      "  \"drop_path_rate\": 0.1,\n",
      "  \"embed_dim\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": [\n",
      "    224,\n",
      "    560\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"mlp_ratio\": 4.0,\n",
      "  \"model_type\": \"donut-swin\",\n",
      "  \"num_channels\": 3,\n",
      "  \"num_heads\": [\n",
      "    4,\n",
      "    8,\n",
      "    16,\n",
      "    32\n",
      "  ],\n",
      "  \"num_layers\": 4,\n",
      "  \"patch_size\": 4,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_absolute_embeddings\": false,\n",
      "  \"window_size\": 7\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.mbart.modeling_mbart.MBartForCausalLM'> is overwritten by shared decoder config: MBartConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 10,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"max_length\": 800,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.47.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "model_dir = 'model_dir'\n",
    "model_name = \"Norm/nougat-latex-base\"\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_dir)\n",
    "latex_processor = NougatLaTexProcessor.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "decoder_input_ids = tokenizer(\n",
    "    tokenizer.bos_token, add_special_tokens=False, return_tensors=\"pt\"\n",
    ").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import PIL.Image as Image\n",
    "\n",
    "from array import array\n",
    "\n",
    "def readimage(path):\n",
    "    count = os.stat(path).st_size / 2\n",
    "    with open(path, \"rb\") as f:\n",
    "        return bytearray(f.read())\n",
    "    \n",
    "image_bytes = readimage('../sample-images/lt-2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = Image.open(io.BytesIO(image_bytes))\n",
    "px_val = latex_processor(image).pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-2.117904 , -2.117904 , -2.117904 , ..., -2.117904 ,\n",
       "         -2.117904 , -2.117904 ],\n",
       "        [-2.117904 , -2.117904 , -2.117904 , ..., -2.117904 ,\n",
       "         -2.117904 , -2.117904 ],\n",
       "        [-2.117904 , -2.117904 , -2.117904 , ..., -2.117904 ,\n",
       "         -2.117904 , -2.117904 ],\n",
       "        ...,\n",
       "        [-2.117904 , -2.117904 , -2.117904 , ..., -2.117904 ,\n",
       "         -2.117904 , -2.117904 ],\n",
       "        [-2.117904 , -2.117904 , -2.117904 , ..., -2.117904 ,\n",
       "         -2.117904 , -2.117904 ],\n",
       "        [-2.117904 , -2.117904 , -2.117904 , ..., -2.117904 ,\n",
       "         -2.117904 , -2.117904 ]],\n",
       "\n",
       "       [[-2.0357141, -2.0357141, -2.0357141, ..., -2.0357141,\n",
       "         -2.0357141, -2.0357141],\n",
       "        [-2.0357141, -2.0357141, -2.0357141, ..., -2.0357141,\n",
       "         -2.0357141, -2.0357141],\n",
       "        [-2.0357141, -2.0357141, -2.0357141, ..., -2.0357141,\n",
       "         -2.0357141, -2.0357141],\n",
       "        ...,\n",
       "        [-2.0357141, -2.0357141, -2.0357141, ..., -2.0357141,\n",
       "         -2.0357141, -2.0357141],\n",
       "        [-2.0357141, -2.0357141, -2.0357141, ..., -2.0357141,\n",
       "         -2.0357141, -2.0357141],\n",
       "        [-2.0357141, -2.0357141, -2.0357141, ..., -2.0357141,\n",
       "         -2.0357141, -2.0357141]],\n",
       "\n",
       "       [[-1.8044444, -1.8044444, -1.8044444, ..., -1.8044444,\n",
       "         -1.8044444, -1.8044444],\n",
       "        [-1.8044444, -1.8044444, -1.8044444, ..., -1.8044444,\n",
       "         -1.8044444, -1.8044444],\n",
       "        [-1.8044444, -1.8044444, -1.8044444, ..., -1.8044444,\n",
       "         -1.8044444, -1.8044444],\n",
       "        ...,\n",
       "        [-1.8044444, -1.8044444, -1.8044444, ..., -1.8044444,\n",
       "         -1.8044444, -1.8044444],\n",
       "        [-1.8044444, -1.8044444, -1.8044444, ..., -1.8044444,\n",
       "         -1.8044444, -1.8044444],\n",
       "        [-1.8044444, -1.8044444, -1.8044444, ..., -1.8044444,\n",
       "         -1.8044444, -1.8044444]]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "px_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_data = torch.tensor(np.array([px_val[0],px_val[0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 224, 560])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "decoder_strt_inputs = torch.tensor(np.array([decoder_input_ids[0], decoder_input_ids[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [0]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_strt_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = model.generate(\n",
    "    input_data,\n",
    "    decoder_input_ids=decoder_strt_inputs,\n",
    "    max_length=model.decoder.config.max_length,\n",
    "    early_stopping=True,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=True,\n",
    "    num_beams=5,\n",
    "    bad_words_ids=[[tokenizer.unk_token_id]],\n",
    "    return_dict_in_generate=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,    82, 13727,   113,    82,   707,   113,    39,   115,   113,\n",
       "            41,   115,   115,    82,  1459,    82,   867,    30,    44,    82,\n",
       "          1459,   113,    82,   707,   113,    42,   115,   113,    41,   115,\n",
       "           115,    82,   747,    31,     2],\n",
       "        [    0,    82, 13727,   113,    82,   707,   113,    39,   115,   113,\n",
       "            41,   115,   115,    82,  1459,    82,   867,    30,    44,    82,\n",
       "          1459,   113,    82,   707,   113,    42,   115,   113,    41,   115,\n",
       "           115,    82,   747,    31,     2]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\\\textstyle{\\\\frac{1}{3}}\\\\times\\\\left(6\\\\times{\\\\frac{4}{3}}\\\\right)', '\\\\textstyle{\\\\frac{1}{3}}\\\\times\\\\left(6\\\\times{\\\\frac{4}{3}}\\\\right)']\n"
     ]
    }
   ],
   "source": [
    "sequence = tokenizer.batch_decode(outputs.sequences)\n",
    "sequence = [ s.replace(tokenizer.eos_token, \"\").replace(tokenizer.pad_token, \"\").replace(tokenizer.bos_token, \"\") for s in sequence]\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUIg0_EULp0r"
   },
   "source": [
    "# Creating Mar for inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "pwp2UYMqLo9V",
    "outputId": "e9939ffb-df4e-4c60-ce1d-8cd63002b16e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.make_archive('nougat', 'zip', 'nougat_latex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"model_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - Overwriting ../../model-store/img-latex.mar ...\n"
     ]
    }
   ],
   "source": [
    "!torch-model-archiver \\\n",
    "    --model-name img-latex \\\n",
    "    --version 1.0 \\\n",
    "    --serialized-file model_dir/model.safetensors \\\n",
    "    --handler handler.py \\\n",
    "    --extra-files \"nougat.zip,model_dir/config.json,model_dir/generation_config.json\" \\\n",
    "    --export-path ../../model-store --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7sm_UUvKMODu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Saving Config\n",
    "# model.config.save_pretrained(\".\")\n",
    "# # Saving Model\n",
    "# torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "to8q2DwEUu8b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !torch-model-archiver --model-name img-latex \\\n",
    "# --version 1.0 --model-file model.py \\\n",
    "# --serialized-file model.pt \\\n",
    "# --handler handler.py \\\n",
    "# --extra-files \"nougat.zip, config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fhauiSnBVqDi",
    "outputId": "9d0190f5-6950-4cd9-fc01-649e92c4e01c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir ../../model-store/\n",
    "!mv img-latex.mar ../../model-store/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torchserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "04-y7MqLU_Hx",
    "outputId": "badab015-c52f-45a8-de08-7e9df7c112d2",
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Background processes not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2410/402258262.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'torchserve --model-store model-store/ --models img-latex=img-latex.mar --ts-config img-latex/config.properties &'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/ipykernel/zmqshell.py\u001b[0m in \u001b[0;36msystem_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    620\u001b[0m             \u001b[0;31m# os.system() or use ip.system=ip.system_raw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m             \u001b[0;31m# if they really want a background process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Background processes not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;31m# we explicitly do NOT return the subprocess status code, because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Background processes not supported."
     ]
    }
   ],
   "source": [
    "!torchserve --model-store model-store/ --models img-latex=img-latex.mar --ts-config img-latex/config.properties &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchServe is not currently running.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!torchserve --stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading model as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_pt_path = 'model.pt'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load(model_pt_path, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VisionEncoderDecoderModel().load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del LatexHandler\n",
    "del lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from handler import LatexHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Context:\n",
    "    system_properties = {\n",
    "        'model_dir':'model_dir'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = Context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lh = LatexHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lh.initialize(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = [{'data':image_bytes}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lh.preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b = data[0]['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".conda-default:Python",
   "language": "python",
   "name": "conda-env-.conda-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
